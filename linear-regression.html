<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Regression - Complete Guide</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            color: #333;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }
        
        h2 {
            color: #34495e;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin-top: 30px;
        }
        
        h3 {
            color: #2980b9;
            margin-top: 25px;
        }
        
        .formula {
            background: #ecf0f1;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #e74c3c;
            font-family: 'Times New Roman', serif;
            font-size: 16px;
        }
        
        .formula-title {
            font-weight: bold;
            color: #e74c3c;
            margin-bottom: 10px;
        }
        
        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
        }
        
        code {
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }
        
        .assumptions {
            background: #e8f5e8;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #27ae60;
        }
        
        .section {
            margin-bottom: 40px;
        }
        
        .highlight {
            background: #fff3cd;
            padding: 10px;
            border-radius: 5px;
            border-left: 4px solid #ffc107;
            margin: 10px 0;
        }
        
        .output {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 15px;
            border-radius: 5px;
            font-family: monospace;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Linear Regression - Complete Guide</h1>
        
        <div class="section">
            <h2>What is Linear Regression?</h2>
            <p>Linear Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables, assuming this relationship is linear. The basic idea is to fit a straight line (or hyperplane in multiple dimensions) that minimizes the difference between actual and predicted values.</p>
            
            <div class="formula">
                <div class="formula-title">Linear Regression Equation:</div>
                <strong>y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ</strong><br><br>
                Where:<br>
                • β₀ is the intercepter<br>
                • βⱼ is the slope/coefficients
            </div>
            
            <div class="formula">
                <div class="formula-title">Simple Linear Regression:</div>
                <strong>y = mx + c</strong><br><br>
                Where:<br>
                • m = slope/coefficient<br>
                • x = input<br>
                • c = intercepter<br>
                • m is slope of change happening in input magnitude on best fit line
            </div>
        </div>

        <div class="section">
            <h2>Mathematics Behind Linear Regression</h2>
            <div class="formula">
                <div class="formula-title">Goal - Minimize Residual Sum of Squares (RSS):</div>
                <strong>RSS = Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</strong>
            </div>
        </div>

        <div class="section">
            <h2>Outlier Detection Methods</h2>
            
            <h3>a. Detect Outliers using Z-Score</h3>
            <pre><code>import numpy as np
import pandas as pd
from scipy import stats

# Example dataframe
df = pd.DataFrame({"value": [10, 12, 11, 13, 15, 200, 14, 9]})

# Z-score method
df['z_score'] = stats.zscore(df['value'])
outliers_z = df[df['z_score'].abs() > 3]
print(outliers_z)</code></pre>

            <h3>b. Detect Outliers using IQR Method</h3>
            <pre><code>Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = df[(df['value'] < lower_bound) | (df['value'] > upper_bound)]
print(outliers_iqr)</code></pre>

            <div class="output">
                   value   z_score
5    200  2.644554
            </div>

            <h3>c. Detect Outliers Using Regression Residuals</h3>
            <pre><code>from sklearn.linear_model import LinearRegression

# Example regression dataset
X = np.array([1,2,3,4,5,6,7,8]).reshape(-1,1)
y = np.array([2,4,5,4,5,100,6,5])

model = LinearRegression()
model.fit(X, y)

# Calculate residuals
y_pred = model.predict(X)
residuals = y - y_pred

# Outliers: residuals > 2 standard deviations
threshold = 2 * np.std(residuals)
outliers_residuals = np.where(abs(residuals) > threshold)
print(outliers_residuals)</code></pre>

            <div class="output">(array(,[object Object],),)</div>
        </div>

        <div class="section">
            <h2>Feature Scaling in Regression</h2>
            <p>Feature Scaling refers to the process of transforming your input features so they are on a similar scale. This is important because many regression techniques (especially those involving regularization like Ridge and Lasso) are sensitive to the magnitude of feature values.</p>
            
            <div class="highlight">
                <strong>When to use:</strong> Apply feature scaling using StandardScaler or MinMaxScaler before linear regression, especially when features differ in scale or when using gradient descent or regularization.
            </div>

            <pre><code>from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Example data
X = df.drop("target", axis=1)
y = df["target"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit Linear Regression
model = LinearRegression()
model.fit(X_train_scaled, y_train)
print(model.coef_)</code></pre>
        </div>

        <div class="section">
            <h2>Basic Assumptions of Linear Regression</h2>
            <div class="assumptions">
                <h3>Four Key Assumptions:</h3>
                <ol>
                    <li><strong>Linearity:</strong> The relationship between X and the mean of Y is linear</li>
                    <li><strong>Homoscedasticity:</strong> The variance of residual is the same for any value of X</li>
                    <li><strong>Independence:</strong> Observations are independent of each other</li>
                    <li><strong>Normality:</strong> For any fixed value of X, Y is normally distributed</li>
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>Regularization</h2>
            <p>Regularization is a technique used in machine learning to prevent overfitting, which otherwise causes models to perform poorly on unseen data. By adding a penalty for complexity, regularization encourages simpler and more generalizable models.</p>
            
            <h3>1. Lasso Regression</h3>
            <p>It stands for (Least Absolute Shrinkage and Selection Operator) regression. It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function.</p>
            
            <div class="formula">
                <div class="formula-title">Lasso Cost Function:</div>
                <strong>Cost Function = RSS + λ Σⱼ₌₁ᵖ |βⱼ|</strong><br><br>
                Where:<br>
                <strong>RSS = Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</strong>
            </div>
            
            <div class="highlight">
                <strong>Use Case:</strong> When you suspect many irrelevant features. This penalty can shrink some coefficients to zero which helps in feature selection and ignoring the less important ones.
            </div>
        </div>

        <div class="section">
            <h2>Gradient Descent</h2>
            <p>An algorithm that slowly adjusts m and c to reduce the cost step by step.</p>
            
            <div class="highlight">
                <strong>Analogy:</strong><br>
                • Cost Function = hill<br>
                • Gradient Descent = walking downhill to reach minimum point<br>
                • Minimum point = best values of m and c<br>
                • Best m & c = best fit line
            </div>

            <h3>Convergence Theorem</h3>
            <p>The convergence theorem in gradient descent states: If the cost function is convex and the learning rate α is small enough, then gradient descent is guaranteed to converge to the global minimum.</p>
            
            <div class="formula">
                <div class="formula-title">Update Rule:</div>
                <strong>m = m - α(∂J/∂m)</strong><br>
                <strong>c = c - α(∂J/∂c)</strong><br><br>
                Where:<br>
                • α = learning rate<br>
                • ∂J/∂m = slope of cost function w.r.t m<br>
                • ∂J/∂c = slope of cost function w.r.t c
            </div>
        </div>

        <div class="section">
            <h2>Checking for Overfitting or Underfitting</h2>
            <p>To check whether a model is overfitting or underfitting, I primarily compare the model's performance on the training set vs. validation/test set, along with analyzing learning curves and error patterns.</p>
            
            <div class="highlight">
                <strong>Detection Methods:</strong><br>
                • <strong>Overfitting:</strong> Training accuracy is high but validation accuracy drops significantly<br>
                • <strong>Underfitting:</strong> Both training and validation accuracies are low<br>
                • <strong>Learning Curves:</strong> Divergence indicates overfitting, while parallel high losses indicate underfitting
            </div>
        </div>

        <div class="section">
            <h2>Interpreting P-values in Regression</h2>
            <p>In regression, p-values tell us whether a predictor variable has a statistically significant relationship with the target variable, after controlling for other variables in the model.</p>
            
            <div class="highlight">
                <strong>P-value Interpretation:</strong><br>
                • A p-value tests the null hypothesis that the coefficient equals zero<br>
                • If p < 0.05: reject the null and conclude that the predictor has a meaningful relationship<br>
                • If p ≥ 0.05: the variable may not be contributing significantly<br>
                • Also consider multicollinearity, effect size, and business relevance
            </div>

            <pre><code>import statsmodels.api as sm
import pandas as pd

# Example data
df = pd.DataFrame({
    "X": [1,2,3,4,5,6,7,8],
    "y": [3,4,6,8,11,13,14,17]
})

X = df["X"]
y = df["y"]

# Add intercept manually
X = sm.add_constant(X)

# Fit model
model = sm.OLS(y, X).fit()

# Print summary (contains p-values)
print(model.summary())</code></pre>

            <div class="output">
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.989
Model:                            OLS   Adj. R-squared:                  0.988
Method:                 Least Squares   F-statistic:                     554.7
Date:                Wed, 10 Dec 2025   Prob (F-statistic):           3.84e-07
Time:                        09:22:01   Log-Likelihood:                -5.6112
No. Observations:                   8   AIC:                             15.22
Df Residuals:                       6   BIC:                             15.38
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.2857      0.439      0.651      0.539      -0.789       1.360
X              2.0476      0.087     23.552      0.000       1.835       2.260
==============================================================================
            </div>
        </div>

        <div class="section">
            <h2>Dealing with Outliers in Linear Regression</h2>
            <p>Outliers can strongly affect linear regression because Ordinary Least Squares (OLS) is sensitive to squared errors. I first detect them using methods like IQR, Z-score. Then I understand whether the outlier is an error or a genuine extreme case.</p>
            
            <div class="highlight">
                <strong>Treatment Options:</strong><br>
                • Transform the variable<br>
                • Winsorize or remove outliers<br>
                • Use robust methods like Huber or RANSAC regression<br>
                • If outliers are meaningful, prefer switching to models or loss functions that reduce their influence rather than simply deleting them
            </div>
        </div>
    </div>
</body>
</html>