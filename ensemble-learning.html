<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bagging & Boosting Techniques</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.2);
        }

        h1 {
            color: #667eea;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 15px;
        }

        h2 {
            color: #764ba2;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.8em;
            border-left: 5px solid #764ba2;
            padding-left: 15px;
        }

        h3 {
            color: #667eea;
            margin-top: 25px;
            margin-bottom: 10px;
            font-size: 1.4em;
        }

        h4 {
            color: #555;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .intro-box {
            background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #667eea;
        }

        .ensemble-types {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .ensemble-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
            transition: transform 0.3s, box-shadow 0.3s;
        }

        .ensemble-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
        }

        .ensemble-card h3 {
            margin-top: 0;
        }

        .formula {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 3px solid #667eea;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }

        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
            border: 2px solid #667eea;
        }

        code {
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }

        .image-placeholder {
            text-align: center;
            margin: 25px 0;
        }

        .image-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
            height: 300px;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 10px;
        }

        .comparison-box {
            background: #fff3cd;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #ffc107;
        }

        .highlight {
            background: #fffacd;
            padding: 2px 6px;
            border-radius: 3px;
        }

        .section {
            margin-bottom: 40px;
        }

        .key-points {
            background: #e7f3ff;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #2196F3;
        }

        .architecture-steps {
            counter-reset: step-counter;
            list-style: none;
            margin-left: 0;
        }

        .architecture-steps li {
            counter-increment: step-counter;
            margin-bottom: 15px;
            padding-left: 40px;
            position: relative;
        }

        .architecture-steps li:before {
            content: counter(step-counter);
            position: absolute;
            left: 0;
            top: 0;
            background: #667eea;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéØ Bagging & Boosting Techniques</h1>

        <div class="intro-box">
            <p><strong>Ensembles solve overfitting the training data or underperformance on unseen instances by aggregating models and balancing their errors.</strong></p>
        </div>

        <div class="section">
            <h2>üìä Type of Ensemble</h2>
            
            <div class="ensemble-types">
                <div class="ensemble-card">
                    <h3>1. Bagging</h3>
                    <p>It is a homogeneous weak learners' model that learns from each other independently in parallel and combines them for determining the model average.</p>
                    <p><strong>Example Algorithms:</strong> Random Forest (most famous bagging-based algorithm).</p>
                </div>

                <div class="ensemble-card">
                    <h3>2. Boosting</h3>
                    <p>It is also a homogeneous weak learners' model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.</p>
                    <p><strong>Example Algorithms:</strong> AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.</p>
                </div>
            </div>
        </div>

        <div class="image-placeholder">
            <img src="https://github.com/dhanushupgrad/ModelDiagram/blob/main/ensemble%20types.png?raw=true" alt="Ensemble Methods Diagram" height="300px">
        </div>

        <div class="section">
            <h2>1Ô∏è‚É£ Bagging</h2>
            
            <p>It is an ensemble technique used to reduce variance and improve model stability.</p>
            
            <p>It works by creating multiple training datasets through <span class="highlight">row sampling with replacement</span> ‚Äî known as bootstrap sampling ‚Äî from the original dataset.</p>

            <p>Each of these samples is used to train a separate model independently and in parallel. Once trained, the predictions from all models are aggregated ‚Äî typically using majority voting for classification or averaging for regression. This helps reduce overfitting and makes the overall model more robust.</p>

            <div class="key-points">
                <h4>Row sampling with replacement:</h4>
                <p>In bagging means creating bootstrap datasets by randomly drawing rows from the original dataset. Allowing duplicates, so each model trains on a slightly different dataset. This diversity among models is what makes bagging powerful.</p>
            </div>

            <h3>Random Forest (Classifier/Regressor)</h3>
            
            <p>In random forest we will use decision tree for training the subset over the row sampling with replacement along with feature sampling.</p>
            
            <p>Because we are using final majority vote for final output, it will reduce the high variance to low variance with multiple decision trees.</p>
            
            <p>It will not affect when small dataset comes again for training.</p>
            
            <p>In Regression problem on final output eg=[2.8, 0.99] it will take mean or median for final output.</p>

            <pre><code>from sklearn.ensemble import RandomForestClassifier

            model = RandomForestClassifier(n_estimators=100, max_depth=10)
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)</code></pre>
        </div>

        <div class="image-placeholder">
            <img src="https://github.com/dhanushupgrad/ModelDiagram/blob/main/soving%20diagram%20for%20random%20forest.png?raw=true" alt="Bagging Process Illustration">
        </div>

        <div class="section">
            <h2>2Ô∏è‚É£ Boosting</h2>
            
            <p>Boosting builds an ensemble of models sequentially where each new model learns from the mistakes of the previous ones. It focuses on hard-to-predict examples and gradually improves overall accuracy.</p>
            
            <div class="key-points">
                <ul>
                    <li>Usually aggregation done by weighted voting or weighted averaging</li>
                    <li>No replacement; uses full data but reweights samples</li>
                    <li>Reduce bias and improve accuracy</li>
                </ul>
            </div>

            <h3>a. AdaBoost</h3>
            
            <p>It is also known as adaptive boosting technique. Its intuition is error-focused, and its mathematics is weight-driven. Usually uses shallow decision trees.</p>

            <h4>Mathematical Formulation</h4>

            <ol>
                <li><strong>Initialize Weights:</strong></li>
            </ol>
            <div class="formula">
                w<sub>i</sub> = 1/N (uniformly distributed for each sample i)
            </div>

            <ol start="2">
                <li><strong>Train Weak Learner:</strong></li>
            </ol>
            <div class="formula">
                Train model h<sub>t</sub>(x) on weighted data
            </div>

            <ol start="3">
                <li><strong>Compute Error:</strong></li>
            </ol>
            <div class="formula">
                Œµ<sub>t</sub> = Œ£<sub>i=1 to N</sub> w<sub>i</sub> ¬∑ I(h<sub>t</sub>(x<sub>i</sub>) ‚â† y<sub>i</sub>)
                <br><small>Where I is the indicator function</small>
            </div>

            <ol start="4">
                <li><strong>Compute Model Weight:</strong></li>
            </ol>
            <div class="formula">
                Œ±<sub>t</sub> = (1/2) ¬∑ ln((1 - Œµ<sub>t</sub>) / Œµ<sub>t</sub>)
                <br><small>Higher accuracy ‚Üí higher Œ±<sub>t</sub></small>
            </div>

            <ol start="5">
                <li><strong>Update Sample Weights:</strong></li>
            </ol>
            <div class="formula">
                w<sub>i</sub> ‚Üê w<sub>i</sub> ¬∑ e<sup>-Œ±<sub>t</sub> ¬∑ y<sub>i</sub> ¬∑ h<sub>t</sub>(x<sub>i</sub>)</sup>
                <br><small>Misclassified samples ‚Üí weight increases</small>
                <br><small>Correctly classified ‚Üí weight decreases</small>
                <br><small>Normalize weights so they sum to 1</small>
            </div>

            <ol start="6">
                <li><strong>Final Strong Classifier:</strong></li>
            </ol>
            <div class="formula">
                H(x) = sign(Œ£<sub>t=1 to T</sub> Œ±<sub>t</sub> ¬∑ h<sub>t</sub>(x))
            </div>

            <pre><code>from sklearn.ensemble import AdaBoostClassifier

            # Initialize the AdaBoost model
            model = AdaBoostClassifier(
                n_estimators=100,  # number of weak learners (default: 50)
                learning_rate=1.0  # controls contribution of each classifier
            )

            # Train the model
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)</code></pre>

            <div class="image-placeholder">
                <img src="https://raw.githubusercontent.com/dhanushupgrad/ModelDiagram/85ef460af615ca386b33ac63c6c4a794aef45c40/adaboost%20architecture.png" alt="AdaBoost Process">
            </div>

            <h3>b. Gradient Boosting</h3>
            
            <p>Build a strong model by sequentially adding weak learners that correct the residual errors of previous models.</p>
            
            <p>Each new model fits the gradient of the loss function ‚Äî i.e., it learns to minimize the error directly.</p>
            
            <div class="comparison-box">
                <p><strong>Key Difference:</strong> AdaBoost reweights samples; Gradient Boosting fits residuals using gradient descent.</p>
                <p>Residual errors via gradient uses weak learners (e.g., shallow trees).</p>
            </div>

            <p>GradientBoostingClassifier builds trees sequentially, like AdaBoost, but instead of reweighting samples, it fits each new tree to the residual errors of the previous ensemble (gradient descent on loss function).</p>

            <h4>üèó Architecture of Gradient Boosting</h4>
            
            <ol class="architecture-steps">
                <li>Start with a base prediction (e.g., mean for regression)</li>
                <li>Compute residuals (errors between prediction and actual)</li>
                <li>Train a weak learner (e.g., decision tree) to predict these residuals</li>
                <li>Add this learner's output to the previous prediction</li>
                <li>Repeat for T rounds</li>
                <li>Final prediction is the sum of all learners, each scaled by a learning rate</li>
            </ol>

            <pre><code>from sklearn.ensemble import GradientBoostingClassifier

            # Initialize the Gradient Boosting model
            model = GradientBoostingClassifier(
                n_estimators=100,   # number of boosting stages
                learning_rate=0.1,  # step size shrinkage
                max_depth=3         # depth of individual trees
            )

            # Train the model
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)</code></pre>

            <div class="image-placeholder">
                <img src="https://raw.githubusercontent.com/dhanushupgrad/ModelDiagram/2237d1f13cec58ca87d56a5d7ab95ae7d7454acb/gradient%20descent%20architecture.png" alt="Gradient Boosting Architecture">
            </div>

            <h3>c. XGBoost</h3>
            
            <p>XGBoost (Extreme Gradient Boosting) is an advanced, regularized version of gradient boosting that builds trees sequentially to minimize error, using second-order optimization and shrinkage for better accuracy and generalization.</p>
            
            <p>It's an optimized implementation of gradient boosting with additional regularization (L1/L2) to reduce overfitting.</p>

            <h4>Key Enhancements over Gradient Boosting:</h4>
            <ul>
                <li><strong>Regularization:</strong> Penalizes model complexity to prevent overfitting</li>
                <li><strong>Second-order Taylor approximation:</strong> Uses both gradient and Hessian (curvature) for more precise updates</li>
                <li><strong>Shrinkage (learning rate):</strong> Scales each tree's contribution</li>
                <li><strong>Column subsampling:</strong> Adds randomness to improve generalization</li>
                <li><strong>Parallelization:</strong> Efficient tree construction and scalability</li>
            </ul>

<pre><code>from xgboost import XGBClassifier

# Initialize the XGBoost model
model = XGBClassifier(
    n_estimators=100,      # number of boosting rounds
    learning_rate=0.1,     # step size shrinkage
    max_depth=3,           # depth of individual trees
    subsample=0.8,         # fraction of samples used per tree
    colsample_bytree=0.8,  # fraction of features used per tree
    random_state=42
)

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)</code></pre>

            <div class="image-placeholder">
                <img src="default.png" alt="XGBoost Architecture">
            </div>
        </div>

        <div class="section">
            <h2>üîç Important Properties of Random Forest Classifiers</h2>
            
            <div class="comparison-box">
                <ol>
                    <li><strong>Decision Tree:</strong> Low Bias and High Variance</li>
                    <li><strong>Ensemble Bagging (Random Forest Classifier):</strong> Low Bias and Low Variance</li>
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>‚öñÔ∏è AdaBoost vs Gradient Boosting vs XGBoost</h2>

            <div class="ensemble-types">
                <div class="ensemble-card">
                    <h4>a. AdaBoost</h4>
                    <p><strong>Key phrase:</strong> "Adaptive Boosting adjusts sample weights to emphasize misclassified points."</p>
                    <p><strong>Weakness:</strong> Sensitive to outliers and noisy data.</p>
                    <p><strong>Use case:</strong> Good for simple datasets where interpretability matters.</p>
                </div>

                <div class="ensemble-card">
                    <h4>b. Gradient Boosting</h4>
                    <p><strong>Key phrase:</strong> "Gradient Boosting builds trees sequentially, each correcting residual errors of the previous."</p>
                    <p><strong>Weakness:</strong> Can overfit if n_estimators or max_depth are too high.</p>
                    <p><strong>Use case:</strong> Balanced choice for structured/tabular data.</p>
                </div>

                <div class="ensemble-card">
                    <h4>c. XGBoost</h4>
                    <p><strong>Key phrase:</strong> "XGBoost is Gradient Boosting with system-level optimizations and regularization."</p>
                    <p><strong>Strength:</strong> Fast, accurate, widely used in Kaggle competitions and industry.</p>
                    <p><strong>Use case:</strong> Large datasets, production ML pipelines, when speed and accuracy are critical.</p>
                </div>
            </div>
        </div>
    </div>
</body>
</html>