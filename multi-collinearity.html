<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multiple Linear Regression & Multicollinearity</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1100px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        
        h1 {
            color: #0077b6;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            border-bottom: 3px solid #0077b6;
            padding-bottom: 15px;
        }
        
        h2 {
            color: #00b4d8;
            margin-top: 35px;
            margin-bottom: 20px;
            font-size: 1.8em;
            border-left: 5px solid #00b4d8;
            padding-left: 15px;
        }
        
        h3 {
            color: #023e8a;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        h4 {
            color: #0096c7;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        
        .intro-box {
            background: #e0f7ff;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #0077b6;
        }
        
        .definition-box {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #00b4d8;
        }
        
        .formula {
            background: #e3f2fd;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            font-size: 1.3em;
            text-align: center;
            border: 2px solid #2196F3;
            font-family: 'Courier New', monospace;
        }
        
        .method-box {
            background: #f5f5f5;
            padding: 25px;
            margin: 20px 0;
            border-radius: 10px;
            border-left: 5px solid #0077b6;
        }
        
        .method-number {
            background: #00b4d8;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            display: inline-block;
            font-weight: bold;
            margin-bottom: 15px;
        }
        
        .techniques-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .technique-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #00b4d8;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .technique-card h4 {
            color: #0077b6;
            margin-top: 0;
            margin-bottom: 10px;
        }
        
        .highlight {
            background: #d1f2eb;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #28a745;
        }
        
        .warning {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #ffc107;
        }
        
        .info-box {
            background: #cfe2ff;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #0d6efd;
        }
        
        pre {
            background: #2d2d2d;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
        }
        
        code {
            color: #f8f8f2;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            display: block;
            padding: 20px;
            line-height: 1.5;
        }
        
        .output-box {
            background: #f0f0f0;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            border-left: 4px solid #666;
            font-size: 0.9em;
        }
        
        .data-table {
            background: white;
            border-radius: 8px;
            overflow: hidden;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .data-table table {
            width: 100%;
            border-collapse: collapse;
        }
        
        .data-table th {
            background: #0077b6;
            color: white;
            padding: 12px;
            text-align: left;
        }
        
        .data-table td {
            padding: 10px 12px;
            border-bottom: 1px solid #ddd;
        }
        
        .data-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        ul {
            margin-left: 30px;
            margin-top: 10px;
        }
        
        ul li {
            margin: 8px 0;
            color: #444;
        }
        
        .key-points {
            background: #fff9e6;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border: 2px solid #ffb700;
        }
        
        .key-points h4 {
            color: #f57c00;
            margin-top: 0;
        }
        
        .regression-output {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            border: 2px solid #6c757d;
        }
        
        .regression-output h4 {
            color: #495057;
            margin-bottom: 15px;
            font-family: 'Segoe UI', sans-serif;
        }
        
        .stat-row {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 10px 0;
        }
        
        sub {
            font-size: 0.7em;
            vertical-align: sub;
        }
        
        sup {
            font-size: 0.7em;
            vertical-align: super;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üìà Multiple Linear Regression & Multicollinearity</h1>
        
        <h2>P-Value</h2>
        <div class="intro-box">
            <p>In regression, the <strong>p-value</strong> tells you whether a predictor (feature) has a statistically significant relationship with the dependent variable.</p>
            <p style="margin-top: 10px;">The p-value tests the <strong>null hypothesis</strong> that the coefficient of a predictor is equal to zero (no effect).</p>
        </div>
        
        <div class="key-points">
            <h4>So for each variable:</h4>
            <ul>
                <li><strong>Low p-value (&lt; 0.05)</strong> ‚Üí The feature significantly affects Salary</li>
                <li><strong>High p-value (&gt; 0.05)</strong> ‚Üí The feature does not significantly affect Salary</li>
            </ul>
        </div>
        
        <h2>Multi Linear Regression</h2>
        <div class="definition-box">
            <p><strong>Multiple Linear Regression</strong> is a method that models the linear relationship between one dependent variable and multiple independent variables by fitting a linear equation to the observed data.</p>
        </div>
        
        <div class="formula">
            Y = Œ≤<sub>0</sub> + Œ≤<sub>1</sub>X<sub>1</sub> + Œ≤<sub>2</sub>X<sub>2</sub> + ... + Œ≤<sub>n</sub>X<sub>n</sub> + Œµ
        </div>
        
        <pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/sample_data/Salary_Data.csv')
df.head()</code></pre>
        
        <div class="data-table">
            <table>
                <thead>
                    <tr>
                        <th></th>
                        <th>YearsExperience</th>
                        <th>Age</th>
                        <th>Salary</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>0</strong></td>
                        <td>1.1</td>
                        <td>21.0</td>
                        <td>39343</td>
                    </tr>
                    <tr>
                        <td><strong>1</strong></td>
                        <td>1.3</td>
                        <td>21.5</td>
                        <td>46205</td>
                    </tr>
                    <tr>
                        <td><strong>2</strong></td>
                        <td>1.5</td>
                        <td>21.7</td>
                        <td>37731</td>
                    </tr>
                    <tr>
                        <td><strong>3</strong></td>
                        <td>2.0</td>
                        <td>22.0</td>
                        <td>43525</td>
                    </tr>
                    <tr>
                        <td><strong>4</strong></td>
                        <td>2.2</td>
                        <td>22.2</td>
                        <td>39891</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h2>How to Detect Multicollinearity</h2>
        
        <div class="info-box">
            <strong>You can mention practical techniques you use:</strong>
        </div>
        
        <div class="techniques-grid">
            <div class="technique-card">
                <h4>1Ô∏è‚É£ Variance Inflation Factor (VIF)</h4>
                <p><strong>VIF &gt; 5</strong> (or sometimes &gt;10) indicates high multicollinearity.</p>
                <p style="margin-top: 10px; color: #28a745; font-weight: bold;">This is the most common and accepted method in interviews.</p>
            </div>
            
            <div class="technique-card">
                <h4>2Ô∏è‚É£ Correlation Matrix / Heatmap</h4>
                <p>Pairwise Pearson correlation <strong>&gt; 0.8 or 0.9</strong> signals potential multicollinearity.</p>
            </div>
            
            <div class="technique-card">
                <h4>3Ô∏è‚É£ Condition Number</h4>
                <p>High condition number <strong>(&gt;30)</strong> suggests multicollinearity from combined linear dependencies.</p>
            </div>
            
            <div class="technique-card">
                <h4>4Ô∏è‚É£ Unstable Coefficient Signs</h4>
                <p>Large changes in coefficients when adding/removing variables also indicates multicollinearity.</p>
            </div>
        </div>
        
        <h2>Detection Methods with Code</h2>
        
        <div class="method-box">
            <div class="method-number">Method 1</div>
            <h3>Variance Inflation Factor (VIF)</h3>
            
            <pre><code>import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df.drop(columns=["Salary"])
X = sm.add_constant(X)

vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["variable"] = X.columns

print(vif)</code></pre>
            
            <div class="output-box">
         VIF           variable
0  483.802531           const
1   39.490712  YearsExperience
2   39.490712             Age
            </div>
            
            <div class="warning">
                <strong>‚ö†Ô∏è Interpretation:</strong> Both YearsExperience and Age have VIF ‚âà 39.5, which is much higher than the threshold of 5-10, indicating <strong>severe multicollinearity</strong>.
            </div>
        </div>
        
        <div class="method-box">
            <div class="method-number">Method 2</div>
            <h3>Correlation Matrix / Heatmap</h3>
            
            <pre><code>df_corr = df.corr()
threshold = 0.2

high_corr = []
for col1 in df_corr.columns:
    for col2 in df_corr.columns:
        if col1 != col2 and abs(df_corr.loc[col1, col2]) > threshold:
            high_corr.append((col1, col2, df_corr.loc[col1, col2]))

print(high_corr)</code></pre>
            
            <div class="output-box">
[('YearsExperience', 'Age', np.float64(0.9872576107444907)), ...]
            </div>
            
            <div class="warning">
                <strong>‚ö†Ô∏è Interpretation:</strong> YearsExperience and Age have a correlation of <strong>0.987</strong>, which is extremely high (>0.9), confirming severe multicollinearity.
            </div>
        </div>
        
        <div class="method-box">
            <div class="method-number">Method 3</div>
            <h3>Detect Using Condition Number</h3>
            
            <pre><code>import statsmodels.api as sm

X = df.drop(columns=["Salary"], axis=1)
X = sm.add_constant(X)  # add constant for interceptor
y = df["Salary"]

model = sm.OLS(y, X).fit()
print(model.summary())</code></pre>
            
            <div class="regression-output">
                <h4>OLS Regression Results</h4>
                <div class="stat-row">
                    <div><strong>Dep. Variable:</strong> Salary</div>
                    <div><strong>R-squared:</strong> 0.960</div>
                </div>
                <div class="stat-row">
                    <div><strong>Model:</strong> OLS</div>
                    <div><strong>Adj. R-squared:</strong> 0.957</div>
                </div>
                <div class="stat-row">
                    <div><strong>Method:</strong> Least Squares</div>
                    <div><strong>F-statistic:</strong> 323.9</div>
                </div>
                <div class="stat-row">
                    <div><strong>Date:</strong> Tue, 09 Dec 2025</div>
                    <div><strong>Prob (F-statistic):</strong> 1.35e-19</div>
                </div>
                <div class="stat-row">
                    <div><strong>No. Observations:</strong> 30</div>
                    <div><strong>AIC:</strong> 606.7</div>
                </div>
                <div class="stat-row">
                    <div><strong>Df Residuals:</strong> 27</div>
                    <div><strong>BIC:</strong> 610.9</div>
                </div>
                
                <hr style="margin: 20px 0; border: 1px solid #ddd;">
                
                <table style="width: 100%; margin-top: 15px;">
                    <tr style="background: #e9ecef;">
                        <th style="padding: 8px; text-align: left;"></th>
                        <th style="padding: 8px;">coef</th>
                        <th style="padding: 8px;">std err</th>
                        <th style="padding: 8px;">t</th>
                        <th style="padding: 8px;">P>|t|</th>
                    </tr>
                    <tr>
                        <td style="padding: 8px;"><strong>const</strong></td>
                        <td style="padding: 8px;">-6661.9872</td>
                        <td style="padding: 8px;">2.28e+04</td>
                        <td style="padding: 8px;">-0.292</td>
                        <td style="padding: 8px;">0.773</td>
                    </tr>
                    <tr style="background: #f8f9fa;">
                        <td style="padding: 8px;"><strong>YearsExperience</strong></td>
                        <td style="padding: 8px;">6153.3533</td>
                        <td style="padding: 8px;">2337.092</td>
                        <td style="padding: 8px;">2.633</td>
                        <td style="padding: 8px;">0.014</td>
                    </tr>
                    <tr>
                        <td style="padding: 8px;"><strong>Age</strong></td>
                        <td style="padding: 8px;">1836.0136</td>
                        <td style="padding: 8px;">1285.034</td>
                        <td style="padding: 8px;">1.429</td>
                        <td style="padding: 8px;">0.165</td>
                    </tr>
                </table>
                
                <hr style="margin: 20px 0; border: 1px solid #ddd;">
                
                <div class="stat-row">
                    <div><strong>Omnibus:</strong> 2.695</div>
                    <div><strong>Durbin-Watson:</strong> 1.711</div>
                </div>
                <div class="stat-row">
                    <div><strong>Prob(Omnibus):</strong> 0.260</div>
                    <div><strong>Jarque-Bera (JB):</strong> 1.975</div>
                </div>
                <div class="stat-row">
                    <div><strong>Skew:</strong> 0.456</div>
                    <div><strong>Prob(JB):</strong> 0.372</div>
                </div>
                <div class="stat-row">
                    <div><strong>Kurtosis:</strong> 2.135</div>
                    <div><strong style="color: #dc3545;">Cond. No. 626.</strong></div>
                </div>
            </div>
            
            <div class="warning">
                <strong>‚ö†Ô∏è Interpretation:</strong> The Condition Number is <strong>626</strong>, which is much higher than the threshold of 30, confirming multicollinearity.
            </div>
        </div>
        
        <div class="method-box">
            <div class="method-number">Method 4</div>
            <h3>Detect Using Eigenvalues / Eigenvalue Decomposition</h3>
            
            <pre><code>import numpy as np

X = df.drop(columns=["Salary"])
corr_matrix = X.corr()

eigenvalues, eigenvectors = np.linalg.eig(corr_matrix)

print("Eigenvalues:", eigenvalues)</code></pre>
            
            <div class="output-box">
Eigenvalues: [1.98725761 0.01274239]
            </div>
            
            <div class="warning">
                <strong>‚ö†Ô∏è Interpretation:</strong> One eigenvalue is very close to 0 (0.01274), indicating that the features are nearly linearly dependent.
            </div>
        </div>
        
        <h2>Solution: Ridge Regression</h2>
        
        <div class="method-box">
            <div class="method-number">Method 5 (Recommended)</div>
            <h3>Ridge Regression</h3>
            
            <div class="highlight">
                <p><strong>Ridge Regression</strong> adds an <strong>L2 penalty</strong> that shrinks coefficients and reduces the impact of multicollinearity.</p>
                <p style="margin-top: 10px;">When predictors are highly correlated, OLS coefficients become unstable. Ridge stabilizes them by controlling variance, making the model more reliable.</p>
            </div>
            
            <div class="key-points">
                <h4>In OLS:</h4>
                <ul>
                    <li>Highly correlated features ‚Üí coefficients become unstable</li>
                    <li>Standard errors inflate</li>
                    <li>p-values become unreliable</li>
                    <li>Signs of coefficients may flip</li>
                </ul>
            </div>
            
            <p><strong>Ridge Regression fixes this by adding a penalty term:</strong></p>
            
            <div class="formula">
                Loss = Œ£(Y - ≈∂)¬≤ + Œ± Œ£Œ≤¬≤
            </div>
            
            <p style="margin-top: 15px;">The penalty shrinks coefficients, especially those inflated due to multicollinearity.</p>
            
            <pre><code>from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Split data
X = df.drop(columns=["Salary"])
y = df["Salary"]

# Scale features (important for Ridge)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train Ridge model
ridge = Ridge(alpha=1.0)
ridge.fit(X_scaled, y)

print("Coefficients:", ridge.coef_)
print("Intercept:", ridge.intercept_)</code></pre>
            
            <div class="output-box">
Coefficients: [14110.25121113 11938.71730854]
Intercept: 76003.0
            </div>
            
            <div class="highlight">
                <strong>‚úÖ Result:</strong> Ridge Regression produces stable coefficients that are less sensitive to multicollinearity, making the model more reliable for prediction.
            </div>
        </div>
    </div>
</body>
</html>