<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logistic Regression - Complete Guide</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            color: #333;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #e74c3c;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }

        h2 {
            color: #34495e;
            border-left: 4px solid #e74c3c;
            padding-left: 15px;
            margin-top: 30px;
        }

        h3 {
            color: #c0392b;
            margin-top: 25px;
        }

        .formula {
            background: #fdf2f2;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #e74c3c;
            font-family: 'Times New Roman', serif;
            font-size: 16px;
        }

        .formula-title {
            font-weight: bold;
            color: #c0392b;
            margin-bottom: 10px;
        }

        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
        }

        code {
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        .advantages {
            background: #e8f5e8;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #27ae60;
        }

        .disadvantages {
            background: #fdeaea;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #e74c3c;
        }

        .section {
            margin-bottom: 40px;
        }

        .highlight {
            background: #fff3cd;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #ffc107;
            margin: 10px 0;
        }

        .output {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 15px;
            border-radius: 5px;
            font-family: monospace;
            margin: 10px 0;
        }

        .image-container {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .image-caption {
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }

        .assumptions {
            background: #e3f2fd;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #2196f3;
        }

        ol,
        ul {
            padding-left: 25px;
        }

        li {
            margin-bottom: 8px;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Logistic Regression - Complete Guide</h1>

        <div class="section">
            <h2>What is Logistic Regression?</h2>
            <p>Logistic Regression is used for binary classification problems. It's called a regression technique
                because it models a continuous probability using a linear regression equation, then maps that
                probability to a class using the sigmoid function.</p>

            <div class="highlight">
                <strong>Key Point:</strong> In the final output, it returns (0 or 1) which is the probability of the
                continuous variables, not the class directly. It's a combination of linear regression and sigmoid
                function - that linear part is why it's still considered a regression method.
            </div>

            <div class="image-container">
                ![Logistic regression sigmoid function curve showing S-shaped curve with probability values from 0 to 1,
                mathematical visualization with axes labeled, clean educational style](1766137201.jpg?ar=landscape)
                <div class="image-caption">Sigmoid Function: The S-shaped curve that maps any real number to a
                    probability between 0 and 1</div>
            </div>
        </div>

        <div class="section">
            <h2>Why Logistic Regression instead of Linear Regression?</h2>
            <p>By using a linear line, we can find output, but why are we not using it? Because suppose we have new
                outliers, then our best fit line will have much difference that comes into a high error rate.</p>

            <div class="image-container">
                ![Binary classification visualization showing data points separated by logistic regression decision
                boundary, two classes with different colors, clean educational diagram](1766137210.jpg?ar=square)
                <div class="image-caption">Binary Classification: Logistic regression creates a decision boundary to
                    separate different classes</div>
            </div>
        </div>

        <div class="section">
            <h2>Mathematical Foundation</h2>

            <div class="formula">
                <div class="formula-title">Logistic Regression Equation:</div>
                <strong>P(Y=1|X) = 1 / (1 + e^(-(β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ)))</strong>
            </div>

            <div class="formula">
                <div class="formula-title">Logit Function (Log Odds):</div>
                <strong>logit(p) = log(p/(1-p)) = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ</strong>
            </div>

            <h3>How Mathematically Logistic Regression Works</h3>
            <p>If we want to create a best fit line which linearly separates the data points, we need to ensure that our
                cost function must have:</p>

            <div class="formula">
                <div class="formula-title">Starting Goal:</div>
                <strong>max Σᵢ yᵢwᵀxᵢ</strong>
            </div>

            <pre><code>from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, 
                          random_state=42)
print(f"X shape: {X.shape}, y shape: {y.shape}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, 
                                                    random_state=42)</code></pre>
        </div>

        <div class="section">
            <h2>Basic Assumptions of Logistic Regression</h2>
            <div class="assumptions">
                <h3>Key Assumption:</h3>
                <ol>
                    <li><strong>Linear Relationship:</strong> There should be a linear relationship between independent
                        features and the log odds (logit)</li>
                    <li><strong>Independence:</strong> Observations should be independent of each other</li>
                    <li><strong>No Multicollinearity:</strong> Independent variables should not be highly correlated
                    </li>
                    <li><strong>Large Sample Size:</strong> Logistic regression requires a large sample size for stable
                        results</li>
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>Multi-Class Classification in Logistic Regression</h2>
            <p>On the other hand, it's called <strong>One vs Rest</strong> or <strong>One vs All</strong> approach.</p>

            <div class="highlight">
                <strong>One vs Rest Strategy:</strong> For each class, train a binary classifier that distinguishes that
                class from all other classes combined. The class with the highest probability wins.
            </div>

            <pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# Multi-class classification example
X, y = make_classification(n_samples=1000, n_features=4, n_classes=3, 
                          n_informative=3, random_state=42)

# Logistic Regression with One-vs-Rest
model = LogisticRegression(multi_class='ovr', random_state=42)
model.fit(X_train, y_train)

# Predictions and probabilities
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

print(f"Predictions shape: {predictions.shape}")
print(f"Probabilities shape: {probabilities.shape}")</code></pre>
        </div>

        <div class="section">
            <h2>Optimization in Logistic Regression</h2>

            <h3>1. Gradient Descent</h3>
            <p>Update weights using partial derivatives of cost function.</p>

            <div class="formula">
                <div class="formula-title">Weight Update Rule:</div>
                <strong>w = w - α * ∇J(w)</strong><br><br>
                Where:<br>
                • α = learning rate<br>
                • ∇J(w) = gradient of cost function with respect to weights
            </div>

            <h3>2. Regularization</h3>
            <p>Add penalty terms (L1/L2) to avoid overfitting.</p>

            <div class="formula">
                <div class="formula-title">L1 Regularization (Lasso):</div>
                <strong>Cost = Original Cost + λ Σ|wᵢ|</strong>
            </div>

            <div class="formula">
                <div class="formula-title">L2 Regularization (Ridge):</div>
                <strong>Cost = Original Cost + λ Σwᵢ²</strong>
            </div>

            <pre><code>from sklearn.linear_model import LogisticRegression

# L1 Regularization (Lasso)
model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)

# L2 Regularization (Ridge)
model_l2 = LogisticRegression(penalty='l2', C=1.0)

# Elastic Net (combination of L1 and L2)
model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', 
                                  C=1.0, l1_ratio=0.5)

# Fit models
model_l1.fit(X_train, y_train)
model_l2.fit(X_train, y_train)
model_elastic.fit(X_train, y_train)</code></pre>
        </div>

        <div class="section">
            <h2>Diagnostics & Interpretation in Logistic Regression</h2>

            <div class="image-container">
                ![ROC curve and AUC visualization for logistic regression model evaluation, showing true positive rate
                vs false positive rate, educational style with clear labels](1766137218.jpg?ar=square)
                <div class="image-caption">ROC Curve and AUC: Essential tools for evaluating logistic regression model
                    performance</div>
            </div>

            <h3>Key Evaluation Metrics:</h3>
            <ul>
                <li><strong>Confusion Matrix:</strong> Shows true positives, false positives, true negatives, and false
                    negatives</li>
                <li><strong>Accuracy:</strong> Overall correctness of the model</li>
                <li><strong>Precision:</strong> True positives / (True positives + False positives)</li>
                <li><strong>Recall:</strong> True positives / (True positives + False negatives)</li>
                <li><strong>F1-Score:</strong> Harmonic mean of precision and recall</li>
                <li><strong>ROC Curve & AUC:</strong> Receiver Operating Characteristic and Area Under Curve</li>
                <li><strong>Threshold Tuning:</strong> Adjusting decision threshold for optimal performance</li>
                <li><strong>Odds Ratio Interpretation:</strong> Understanding coefficient meanings</li>
            </ul>

            <pre><code>from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

# Make predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC AUC Score
auc_score = roc_auc_score(y_test, y_pred_proba)
print(f"\nROC AUC Score: {auc_score:.4f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()</code></pre>
        </div>

        <div class="section">
            <h2>Odds Ratio (log odds) Interpretation</h2>
            <p>The coefficients in logistic regression represent the change in log odds for a one-unit increase in the
                predictor variable.</p>

            <div class="formula">
                <div class="formula-title">Odds Ratio:</div>
                <strong>OR = e^β</strong><br><br>
                Where:<br>
                • OR > 1: Positive association<br>
                • OR < 1: Negative association<br>
                    • OR = 1: No association
            </div>

            <pre><code>import numpy as np

# Get coefficients from trained model
coefficients = model.coef_,[object Object],
feature_names = [f'Feature_{i}' for i in range(len(coefficients))]

# Calculate odds ratios
odds_ratios = np.exp(coefficients)

# Display results
for feature, coef, odds_ratio in zip(feature_names, coefficients, odds_ratios):
    print(f"{feature}:")
    print(f"  Coefficient: {coef:.4f}")
    print(f"  Odds Ratio: {odds_ratio:.4f}")
    if odds_ratio > 1:
        print(f"  Interpretation: {odds_ratio:.2f}x more likely for positive class")
    else:
        print(f"  Interpretation: {1/odds_ratio:.2f}x less likely for positive class")
    print()</code></pre>
        </div>

        <div class="section">
            <h2>Advantages of Logistic Regression</h2>
            <div class="advantages">
                <ol>
                    <li><strong>Easy to Understand:</strong> Logistic Regression is very easy to understand and
                        interpret</li>
                    <li><strong>Less Training Required:</strong> It requires less training time compared to other
                        algorithms</li>
                    <li><strong>Good Accuracy:</strong> Good accuracy for many simple datasets and performs well when
                        the dataset is linearly separable</li>
                    <li><strong>No Distribution Assumptions:</strong> It makes no assumptions about distributions of
                        classes in feature space</li>
                    <li><strong>Less Prone to Overfitting:</strong> Logistic regression is less inclined to overfitting,
                        but it can overfit in high dimensional datasets. One may consider Regularization (L1 and L2)
                        techniques to avoid overfitting in these scenarios</li>
                    <li><strong>Easy Implementation:</strong> Logistic regression is easier to implement, interpret, and
                        very efficient to train</li>
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>Disadvantages of Logistic Regression</h2>
            <div class="disadvantages">
                <ol>
                    <li><strong>Feature Engineering Required:</strong> Sometimes a lot of feature engineering is
                        required</li>
                    <li><strong>Correlation Issues:</strong> If the independent features are correlated, it may affect
                        performance</li>
                    <li><strong>Prone to Noise:</strong> It is often quite prone to noise and overfitting</li>
                    <li><strong>Sample Size Limitation:</strong> If the number of observations is lesser than the number
                        of features, Logistic Regression should not be used, otherwise, it may lead to overfitting</li>
                    <li><strong>Linear Limitation:</strong> Non-linear problems can't be solved with logistic regression
                        because it has a linear decision boundary</li>
                    <li><strong>Complex Relationships:</strong> It is tough to obtain complex relationships using
                        logistic regression. More powerful and compact algorithms such as Neural Networks can easily
                        outperform this algorithm</li>
                    <li><strong>Log Odds Linearity:</strong> Logistic Regression needs that independent variables are
                        linearly related to the log odds (log(p/(1-p)))</li>
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>Complete Implementation Example</h2>

            <pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load and prepare data
# Assuming you have a dataset
# df = pd.read_csv('your_dataset.csv')
# X = df.drop('target', axis=1)
# y = df['target']

# For demonstration, using synthetic data
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,
                          n_redundant=2, n_classes=2, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=42)

# Feature scaling (optional but recommended)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the model
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Feature importance (coefficients)
feature_importance = abs(model.coef_,[object Object],)
feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]

for name, importance in zip(feature_names, feature_importance):
    print(f"{name}: {importance:.4f}")</code></pre>
        </div>


    </div>
</body>

</html>